# Pedagogia Dropout Prediction ¬∑ 2025‚Äì2026

Syst√®me modulaire pour **pr√©voir le risque de d√©crochage** √† partir des traces p√©dagogiques (projets, tests unitaires, notes). 
Le pipeline couvre : t√©l√©chargement automatique des donn√©es depuis Google Drive, ingestion des donn√©es, normalisation en CSV, agr√©gation/merge par √©tudiant, puis **production d'un mod√®le** (clustering + pr√©diction s√©quentielle) avec m√©triques.

> Tech: Python 3.12, scikit‚Äëlearn, UMAP, Docker (optionnel).gogia Dropout Prediction ¬∑ 2025‚Äì2026

Syst√®me modulaire pour **pr√©voir le risque de d√©crochage** √† partir des traces p√©dagogiques (projets, tests unitaires, notes). 
Le pipeline couvre : ingestion des donn√©es, normalisation en CSV, agr√©gation/merge par √©tudiant, puis **production d‚Äôun mod√®le** (clustering + pr√©diction s√©quentielle) avec m√©triques.

> Tech: Python 3.12, scikit‚Äëlearn, UMAP, Docker (optionnel).

---

## ‚ú® Points cl√©s

- **Ingestion** depuis les APIs internes (pagination, back‚Äëoff 429, export JSON par activit√©).
- **Normalisation CSV** + **agr√©gation** (verticale par pr√©fixe) + **fusion horizontale** par `email`.
- **Production mod√®le** (script principal) avec param√®tres explicites : `year`, `n_clusters`, `min_cluster_size`, seuils, fen√™tres `w1/w2`, alphas.
- **Architecture simple** : tout le code est sous `src/` avec utilitaires d√©di√©s dans `src/utils/‚Ä¶`.
- **Docker pr√™t √† l‚Äôemploi** (Dockerfile + docker‚Äëcompose.yml).

---

## üìÅ Structure du d√©p√¥t

```
pedagogia-dropout-prediction_2025-2026/
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ data_loading.py                         # Orchestration pipeline (ingestion ‚Üí CSV ‚Üí agr√©gat ‚Üí merge)
‚îÇ  ‚îú‚îÄ model_production_main.py                # Script principal de production du mod√®le
‚îÇ  ‚îî‚îÄ utils/
‚îÇ     ‚îú‚îÄ data_loading_utils/
‚îÇ     ‚îÇ  ‚îú‚îÄ get_data.py                       # T√©l√©charge JSON (r√©sultats ¬´ delivery ¬ª) pour une unit√©/ann√©e
‚îÇ     ‚îÇ  ‚îú‚îÄ all_json_to_csv.py                # Conversion r√©cursive JSON ‚Üí CSV
‚îÇ     ‚îÇ  ‚îú‚îÄ aggregate_csv.py                  # Agr√©gation verticale par ¬´ pr√©fixe ¬ª
‚îÇ     ‚îÇ  ‚îú‚îÄ merged_csv.py                     # Fusion horizontale par email (ordre impos√© si besoin)
‚îÇ     ‚îÇ  ‚îî‚îÄ get_activities.py                 # Utilitaire activit√©s (pagination)
‚îÇ     ‚îî‚îÄ model_production_data_processing_utils.py
‚îú‚îÄ tests.ipynb
‚îú‚îÄ requirements.txt
‚îú‚îÄ Dockerfile
‚îú‚îÄ docker-compose.yml
‚îî‚îÄ README.md
```

---

## üß∞ Pr√©requis

- **Python 3.12+**
- (Optionnel) **Docker** ‚â• 24
- Acc√®s API + **variables d‚Äôenvironnement** (via `.env`) :  
  - `TOKEN_ID`  
  - `TOKEN_PASS`

Cr√©ez un fichier `.env` √† la racine¬†:

```env
TOKEN_ID=xxxxxxxxxxxxxxxx
TOKEN_PASS=yyyyyyyyyyyyyyyy
```

> Les scripts chargent `.env` via `dotenv` et utilisent ces tokens pour l‚Äôappel API.

---

## ÔøΩ T√©l√©chargement automatique des donn√©es

Le syst√®me inclut un **microservice de t√©l√©chargement** qui r√©cup√®re automatiquement le fichier CSV depuis Google Drive.


**via Docker Compose:**
```bash
docker compose up csv-downloader --build
```

Le fichier sera t√©l√©charg√© dans `./data/DATA_2025_pred_proj.csv`.

### Configuration manuelle

Vous pouvez aussi utiliser le microservice directement :

```bash
python src/csv_downloader.py \
  --url "https://drive.google.com/file/d/1ZeJ2f1qfpENc-gIwYGiQsVM5nCGaTQkG/view?usp=drive_link" \
  --output "./data/DATA_2025_pred_proj.csv" \
  --retries 3 \
  --verify
```

---

## ÔøΩüöÄ Installation rapide (sans Docker)

```bash
# 1) Cloner et se placer dans le dossier
git clone <votre-repo> pedagogia-dropout-prediction_2025-2026
cd pedagogia-dropout-prediction_2025-2026

# 2) Cr√©er un venv et installer
python -m venv .venv
# Windows: .venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

pip install -r requirements.txt
```

---

## üîÑ Pipeline de donn√©es (ingestion ‚Üí CSV ‚Üí agr√©gat ‚Üí merge)

Vous pouvez soit appeler les sous‚Äëscripts, soit piloter via `src/data_loading.py`. Ci‚Äëdessous, la version **pas √† pas** (recommand√© pour √™tre explicite sur les chemins).

### 1) T√©l√©charger les JSON (*results ¬´ delivery ¬ª*)

Script¬†: `src/utils/data_loading_utils/get_data.py`

Arguments d√©tect√©s¬†: `--year` (int, requis), `--unit` (str, requis), `--source` (intra/extra, d√©faut *intra*), `--output` (dossier de sortie).

```bash
python -u src/utils/data_loading_utils/get_data.py   --year 2024   --unit B-CPE-100   --source intra   --output data/data_json
```

> ‚ùóÔ∏èSi vous voyez l‚Äôerreur `the following arguments are required: --year, --unit`, passez bien ces 2 param√®tres (ils sont **requis**).

### 2) Convertir JSON ‚Üí CSV

```bash
python -u src/utils/data_loading_utils/all_json_to_csv.py   --input data/data_json   --output data_csv
```

### 3) Agr√©ger verticalement (par ¬´ pr√©fixe ¬ª de fichier)

Le pr√©fixe est tout avant le **dernier** underscore `_`. Tous les fichiers partageant ce pr√©fixe sont concat√©n√©s.

```bash
python -u src/utils/data_loading_utils/aggregate_csv.py   --indir data_csv   --outdir agg   --filter "*.csv"
```

### 4) Fusion horizontale par `email`

```bash
python -u src/utils/data_loading_utils/merged_csv.py   --indir agg   --out data/DATA.csv
```

> √Ä l‚Äôissue de ces 4 √©tapes, vous disposez d‚Äôun **master CSV** : `data/DATA.csv`

---

## ü§ñ Production du mod√®le

Script¬†: `src/model_production_main.py`  
Arguments disponibles¬†: `--year`, `--n_clusters`, `--min_cluster_size`, `--data_file`, `--threshold`, `--w1`, `--w2`, `--alpha1`, `--alpha2`.

Exemple minimal reproductible¬†:

```bash
python -u src/model_production_main.py   --year 2024   --n_clusters 10   --min_cluster_size 20   --data_file data/DATA.csv   --threshold 10.0   --w1 2 --w2 2   --alpha1 0.05 --alpha2 0.05
```

> Notes :
> - `n_clusters` et `min_cluster_size` contr√¥lent le **clustering** amont.
> - `w1/w2` et `alpha1/alpha2` contr√¥lent la **pr√©diction s√©quentielle** (p. ex. variantes SPCI) et les budgets d‚Äôalpha.
> - `threshold` peut servir de seuil m√©tier (ex. d√©cision d‚Äôalerte).

---

## üß™ Reproductibilit√©

- Versions **pinn√©es** dans `requirements.txt`.
- Fixez vos graines globales (ex. `RANDOM_STATE=42`) au niveau des scripts si besoin.
- Exportez vos **artefacts** (mod√®les, m√©triques) dans un dossier versionn√© (`models/`, `reports/`).

---

## üê≥ Ex√©cution via Docker (optionnel)

### Build image

```bash
docker build -t pedagogia-dropout:latest .
```

### Lancer le pipeline dans un conteneur

```bash
docker run --rm -it   --env-file .env   -v "$(pwd)/data:/app/data"   -v "$(pwd)/models:/app/models"   pedagogia-dropout:latest   python -u src/utils/data_loading_utils/get_data.py --year 2024 --unit B-CPE-100 --output data/data_json
```

> Vous pouvez ensuite encha√Æner les √©tapes 2‚Üí4, puis lancer `model_production_main.py` de la m√™me fa√ßon.

> **docker-compose** : un fichier `docker-compose.yml` est fourni ; adaptez les volumes `data/` et `models/` selon votre environnement.

---

## üóÇÔ∏è Donn√©es attendues (r√©sum√©)

- **Entr√©es** : JSON par activit√© \‚Üí CSV normalis√©s \‚Üí agr√©gats par pr√©fixe \‚Üí merge final par `email`.
- **Sortie** : `data/DATA.csv` (master), + artefacts de mod√®les dans `models/` (si configur√©).
