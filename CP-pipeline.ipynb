{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffaf41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions-CP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864b07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions-CP2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d5b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfce070",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 23\n",
    "wac = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c863fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "if wac:\n",
    "    if year == 23:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal_WAC_2023.csv\")\n",
    "    elif year == 24:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal_WAC_2024.csv\")\n",
    "else:\n",
    "    if year == 23:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal2023.csv\")\n",
    "        n_clusters = 5\n",
    "    elif year == 24:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal2024.csv\")\n",
    "        n_clusters = 4\n",
    "df_ = pd.read_csv(CSV_FILE)\n",
    "mask = df_.columns.str.startswith(\"B-CPE-210\")\n",
    "df_ = df_.loc[:, ~mask]\n",
    "df = df_.copy()\n",
    "mark_cols = [c for c in df.columns if c.endswith(\"mark\")]\n",
    "df[mark_cols] = df[mark_cols].div(df[mark_cols].mean())\n",
    "nb_nan_par_ligne = df.isna().sum(axis=1)\n",
    "print(max(nb_nan_par_ligne))\n",
    "if year == 24:\n",
    "    df = df[nb_nan_par_ligne < 495]\n",
    "if year==23:\n",
    "    df = df[nb_nan_par_ligne < 130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9facea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1[\"source\"] = \"real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4fb679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    108\n",
      "1     49\n",
      "2    170\n",
      "3    587\n",
      "4    137\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    # CPE 2024\n",
    "    dfcpool = df1[[c for c in df1.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday\\d+_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    dfcpool_mark = dfcpool[cols_keep]\n",
    "    X = dfcpool_mark.fillna(0)\n",
    "elif year == 23:\n",
    "    # CPE 2023\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df2 = df1.copy()\n",
    "df2['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaaab9",
   "metadata": {},
   "source": [
    "## GAN à tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70cad4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    331\n",
      "1    322\n",
      "2     20\n",
      "3     88\n",
      "4     86\n",
      "5      3\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    # CPE 2024\n",
    "    dfcpool = df1[[c for c in df1.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday\\d+_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    dfcpool_mark = dfcpool[cols_keep]\n",
    "    X = dfcpool_mark.fillna(0)\n",
    "elif year == 23:\n",
    "    # CPE 2023\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "dfganall = df1.copy()\n",
    "dfganall['cluster'] = clusters\n",
    "\n",
    "print(pd.Series(clusters).value_counts().sort_index())\n",
    "counts = dfganall['cluster'].value_counts()\n",
    "least = counts.nsmallest(3).index.tolist()\n",
    "dfganleast = dfganall[dfganall['cluster'].isin(least)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a2fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsolationForest détecte 255 outliers\n",
      "LocalOutlierFactor détecte 255 outliers\n",
      "PCA reconstruction >95% quantile : 255 outliers\n",
      "Détectés par les méthodes : 91 points\n",
      "Détectés par les méthodes : 152 points\n",
      "Détectés par les méthodes : 112 points\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# --- 0) Charger et préparer vos données ---\n",
    "# df : votre DataFrame complet\n",
    "features = [c for c in df.columns if c not in (\"student_id\", \"cluster\", \"email\", \"source\")]\n",
    "X = df1[features].fillna(0).values\n",
    "\n",
    "# Standardisation (cruciale pour la plupart des méthodes)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- 1) Isolation Forest ---\n",
    "iso = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.3,   # fraction estimée d'outliers\n",
    "    random_state=42\n",
    ")\n",
    "y_iso = iso.fit_predict(X_scaled)  \n",
    "# y_iso == -1 → anomalie, 1 → normal\n",
    "outliers_iso = df[y_iso == -1]\n",
    "\n",
    "# --- 2) Local Outlier Factor (LOF) ---\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=10,\n",
    "    contamination=0.3,   # même fraction\n",
    "    novelty=False         # False → on ne peut pas appeler .predict sur de nouvelles données\n",
    ")\n",
    "y_lof = lof.fit_predict(X_scaled)\n",
    "# y_lof == -1 → anomalie, 1 → normal\n",
    "outliers_lof = df[y_lof == -1]\n",
    "\n",
    "# --- 3) Reconstruction Error via PCA ---\n",
    "# On choisit k composantes pour capturer par ex. 90% de variance\n",
    "pca = PCA(n_components=0.9, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_recon = pca.inverse_transform(X_pca)\n",
    "recon_error = np.mean((X_scaled - X_recon)**2, axis=1)\n",
    "\n",
    "# Seuil : on prend les points au‑dessus du 95ème percentile d’erreur\n",
    "thresh = np.percentile(recon_error, 70)\n",
    "outliers_pca = df[recon_error > thresh]\n",
    "\n",
    "# --- 4) Résumé et comparaison ---\n",
    "print(\"IsolationForest détecte\",  len(outliers_iso),  \"outliers\")\n",
    "print(\"LocalOutlierFactor détecte\", len(outliers_lof), \"outliers\")\n",
    "print(\"PCA reconstruction >95% quantile :\", len(outliers_pca), \"outliers\")\n",
    "\n",
    "# Par exemple, les points unanimement détectés\n",
    "common1 = set(outliers_iso.index) & set(outliers_lof.index) #  & set(outliers_pca.index)\n",
    "print(\"Détectés par les méthodes :\", len(common1), \"points\")\n",
    "common2 = set(outliers_iso.index)  & set(outliers_pca.index)\n",
    "print(\"Détectés par les méthodes :\", len(common2), \"points\")\n",
    "common3 = set(outliers_lof.index)  & set(outliers_pca.index)\n",
    "print(\"Détectés par les méthodes :\", len(common3), \"points\")\n",
    "df_common_outliers = df1.loc[list(common2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953fbb2d",
   "metadata": {},
   "source": [
    "## SMOTE like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f268e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0    108\n",
      "1    147\n",
      "2    170\n",
      "3    587\n",
      "4    137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- 0) Préparation de X et des colonnes ---\n",
    "X = df2.copy()\n",
    "X = X.drop(columns=['email', 'source']).fillna(0).reset_index(drop=True)\n",
    "clusters = X['cluster'].values\n",
    "feature_cols = X.columns.drop('cluster')\n",
    "\n",
    "# distinguez numériques / catégorielles\n",
    "style_cols = [c for c in feature_cols if '_style' in c]\n",
    "cat_cols = [c for c in feature_cols if c.endswith('passed')] + style_cols\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "# --- 1) Calcul des distributions par cluster pour chaque catégorielle ---\n",
    "# cluster_cat_probs[cluster][column] = {val: prob, ...}\n",
    "cluster_cat_probs = {}\n",
    "for cl, grp in X.groupby('cluster'):\n",
    "    cluster_cat_probs[cl] = {}\n",
    "    for c in cat_cols:\n",
    "        freqs = grp[c].value_counts(normalize=True).to_dict()\n",
    "        cluster_cat_probs[cl][c] = freqs\n",
    "\n",
    "# --- 2) Paramètres d’augmentation ---\n",
    "minor_counts = X['cluster'].value_counts()\n",
    "th = minor_counts.quantile(0.25)\n",
    "minor_cs = minor_counts[minor_counts < th].index.tolist()\n",
    "\n",
    "\n",
    "# --- 3) Génération des nouveaux points ---\n",
    "rows_aug = []\n",
    "\n",
    "for cl in minor_cs:\n",
    "    idx = np.where(clusters == cl)[0]\n",
    "    Xk = X.loc[idx, feature_cols].values\n",
    "    n_new_per    = 100 // len(idx)\n",
    "    k_neighbors  = min(20, len(idx)-2)\n",
    "    # si trop peu de points, on duplique\n",
    "    if len(Xk) < 2:\n",
    "        for i in idx:\n",
    "            for _ in range(n_new_per):\n",
    "                rows_aug.append(X.loc[i, feature_cols].to_dict())\n",
    "        continue\n",
    "\n",
    "    # voisins sur la partie numérique\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=min(k_neighbors+1, len(Xk)),\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    nbrs.fit(Xk[:, [X.columns.get_loc(c) for c in num_cols]])\n",
    "    neigh_idxs = nbrs.kneighbors(return_distance=False)\n",
    "\n",
    "    for i, xi in enumerate(Xk):\n",
    "        for _ in range(n_new_per):\n",
    "            # choisir un voisin différent\n",
    "            nbr_list = [j for j in neigh_idxs[i] if j != i]\n",
    "            j = np.random.choice(nbr_list)\n",
    "            xj = Xk[j]\n",
    "\n",
    "            # interpolation SMOTE-like pour num_cols\n",
    "            lam = np.random.rand()\n",
    "            num_new = xi[[X.columns.get_loc(c) for c in num_cols]] + \\\n",
    "                      lam * (xj[[X.columns.get_loc(c) for c in num_cols]] - \n",
    "                             xi[[X.columns.get_loc(c) for c in num_cols]])\n",
    "\n",
    "            # génération des variables catégorielles selon distributions\n",
    "            cat_new = {}\n",
    "            for c in cat_cols:\n",
    "                dist = cluster_cat_probs[cl][c]\n",
    "                labels = list(dist.keys())\n",
    "                probs = list(dist.values())\n",
    "                cat_new[c] = np.random.choice(labels, p=probs)\n",
    "\n",
    "            # assemblage de la ligne synthétique\n",
    "            new_row = {c: num_new[k] for k, c in enumerate(num_cols)}\n",
    "            new_row.update(cat_new)\n",
    "            new_row['cluster'] = cl\n",
    "            rows_aug.append(new_row)\n",
    "\n",
    "# --- 4) Création du DataFrame synthétique ---\n",
    "df_synth = pd.DataFrame(rows_aug)\n",
    "df_synth['source'] = 'synth'\n",
    "df_synth['email'] = np.nan\n",
    "\n",
    "# Concaténation finale\n",
    "\n",
    "df4 = pd.concat([df2, df_synth], ignore_index=True)\n",
    "print(df4['cluster'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3f24a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fill_interval_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     x_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;28mmin\u001b[39m(x),\u001b[38;5;28mmax\u001b[39m(x), n_new)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     x_new \u001b[38;5;241m=\u001b[39m fill_interval_random(x, n_new\u001b[38;5;241m=\u001b[39mn_new, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     16\u001b[0m all_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([x, x_new])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre de points après génération :\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fill_interval_random' is not defined"
     ]
    }
   ],
   "source": [
    "dfnum = df.drop(columns=[\"email\"]).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(dfnum)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "proj = pca.fit_transform(X_scaled)\n",
    "x = proj[:, 0]\n",
    "y = proj[:, 1]\n",
    "n_new = 450\n",
    "\n",
    "coeffs = np.polyfit(x, y, deg=3)\n",
    "p = np.poly1d(coeffs)\n",
    "if wac:\n",
    "    x_new = np.linspace(min(x),max(x), n_new)\n",
    "else:\n",
    "    x_new = fill_interval_random(x, n_new=n_new, seed=42)\n",
    "all_x = np.concatenate([x, x_new])\n",
    "print(\"Nombre de points après génération :\", len(all_x))\n",
    "\n",
    "# 1) fit du GPR sur (x.reshape(-1,1), y)\n",
    "kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42).fit(np.asarray(x).reshape(-1,1), np.asarray(y) - p(x))\n",
    "\n",
    "# 2) échantillonnage postérieur pour vos nouveaux x = extra\n",
    "#    sample_y renvoie un tableau (n_samples, n_draws)\n",
    "resid = gpr.sample_y(x_new.reshape(-1,1), n_samples=1, random_state=42)\n",
    "y_new = resid.ravel() + p(x_new)\n",
    "all_y = np.concatenate([y, y_new])\n",
    "\n",
    "X_new_pca = np.vstack([x_new, y_new]).T\n",
    "X_synth_scaled = pca.inverse_transform(X_new_pca)\n",
    "X_synth = scaler.inverse_transform(X_synth_scaled)\n",
    "print(\"on a bien autant de colonnes qu'au départ :\", X_synth.shape[1] == dfnum.shape[1])\n",
    "\n",
    "numeric_cols = dfnum.columns.tolist()\n",
    "df_synth = pd.DataFrame(X_synth, columns=numeric_cols)\n",
    "for col in df.columns.difference(df_synth.columns):\n",
    "    df_synth[col] = np.nan\n",
    "df_synth = df_synth[df.columns]\n",
    "\n",
    "df_synth[\"source\"] = \"synth\"\n",
    "\n",
    "df5 = pd.concat([df1, df_synth], ignore_index=True)\n",
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df5[[c for c in df.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    # CPE 2024\n",
    "    dfcpool = df5[[c for c in df5.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday\\d+_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    dfcpool_mark = dfcpool[cols_keep]\n",
    "    X = dfcpool_mark.fillna(0)\n",
    "elif year == 23:\n",
    "    # CPE 2023\n",
    "    X = df5[[c for c in df5.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df5['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32738ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters trop petits à réaffecter : [2]\n",
      "Nouvelles tailles de clusters :\n",
      " 0    466\n",
      "1    166\n",
      "3    218\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "min_cluster_size = 50\n",
    "\n",
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df2[[c for c in df2.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    dfcpool = df2[[c for c in df2.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday01_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    X = dfcpool[cols_keep].fillna(0)\n",
    "elif year == 23:\n",
    "    X = df2[[c for c in df2.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "else:\n",
    "    raise ValueError(f\"Année inattendue : {year}\")\n",
    "\n",
    "# 2) Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3) Premier KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# 4) Mesure des tailles\n",
    "sizes = pd.Series(labels).value_counts().sort_index()\n",
    "\n",
    "# 5) Réaffectation des points de petits clusters\n",
    "small_clusters = sizes[sizes < min_cluster_size].index.tolist()\n",
    "if small_clusters:\n",
    "    print(f\"Clusters trop petits à réaffecter : {small_clusters}\")\n",
    "    # Pour chaque point des clusters trop petits, on calcule la distance à tous les centroides\n",
    "    for sc in small_clusters:\n",
    "        mask_sc = labels == sc\n",
    "        idxs   = np.where(mask_sc)[0]\n",
    "        for i in idxs:\n",
    "            # distances à TOUTES les centroïdes\n",
    "            dists = np.linalg.norm(X_scaled[i] - centroids, axis=1)\n",
    "            # on interdit de rester dans le même (petit) cluster\n",
    "            dists[sc] = np.inf\n",
    "            # réaffectation vers le plus proche cluster valide\n",
    "            labels[i] = int(np.argmin(dists))\n",
    "    # Optionnel : recompute sizes\n",
    "    sizes = pd.Series(labels).value_counts().sort_index()\n",
    "    print(\"Nouvelles tailles de clusters :\\n\", sizes)\n",
    "\n",
    "# 6) Stockage dans df4\n",
    "df3 = df2.copy()\n",
    "df3[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26f8fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((850, 516), (946, 516), (1300, 516))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape, df4.shape, df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251de4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\".*'force_all_finite' was renamed to 'ensure_all_finite'.*\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7b07cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil KMeans     : 1.645\n",
      "Centres clusters : [0.35220526 2.93743438]\n",
      "Meilleur écart = 0.2683 obtenu pour prop = 0.7980\n",
      "proportion de dropout : 0.32540437678401524\n",
      "0.28105268564221186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ecs = []\n",
    "best = -np.inf\n",
    "best_prop = None\n",
    "\n",
    "# On conserve la moyenne précédente pour la comparer\n",
    "prev_mean = None\n",
    "n_rendus = 1\n",
    "df0 = df.copy()\n",
    "df0 = df0.fillna(0)\n",
    "mark_cols = [c for c in df0.columns if c.endswith(\"mark\")][::-1]\n",
    "\n",
    "def last_marks(row):\n",
    "    vals, cols = [], []\n",
    "    for c in mark_cols:\n",
    "        v = row[c]\n",
    "        if v > 0:\n",
    "            vals.append(v)\n",
    "            cols.append(c)\n",
    "            if len(vals) == n_rendus:\n",
    "                break\n",
    "    return pd.Series({\"cols\": cols, \"vals\": vals})\n",
    "\n",
    "tmp = df0.apply(last_marks, axis=1)\n",
    "df0[[\"last_cols\", \"last_vals\"]] = tmp\n",
    "df0[\"lastvals\"] = df0[\"last_vals\"].apply(lambda r: r[0])\n",
    "def split_by_kmeans(x, random_state=0):\n",
    "    \"\"\"\n",
    "    Coupe la liste x en deux clusters via KMeans (k=2).\n",
    "    Retourne :\n",
    "      - t       : seuil estimé (milieu entre les deux centres)\n",
    "      - labels  : tableau 0/1, 0 = cluster « faible », 1 = cluster « fort »\n",
    "      - centers : centres des deux clusters\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    # reshape pour sklearn\n",
    "    X = x.reshape(-1, 1)\n",
    "    # lancer KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=random_state).fit(X)\n",
    "    labels_km = kmeans.labels_              # étiquettes 0 ou 1\n",
    "    centers = kmeans.cluster_centers_.flatten()\n",
    "    # déterminer quel cluster est le plus « faible »\n",
    "    low_label = np.argmin(centers)\n",
    "    # re-mapper pour que 0 = faible, 1 = fort\n",
    "    labels = (labels_km != low_label).astype(int)\n",
    "    # seuil = milieu entre les deux centres\n",
    "    t = float(np.mean(centers))\n",
    "    return t, labels, centers\n",
    "\n",
    "\n",
    "t, lbl, ctr = split_by_kmeans(df0[\"lastvals\"])\n",
    "print(f\"Seuil KMeans     : {t:.3f}\")\n",
    "print(f\"Centres clusters : {ctr}\")\n",
    "for i, prop in enumerate(np.linspace(0.5, 1, 100)):\n",
    "    threshold = min(ctr) * prop\n",
    "    # indicatrice binaire\n",
    "    y_all = (df0[\"lastvals\"] < threshold).astype(int)\n",
    "    current_mean = y_all.mean()\n",
    "\n",
    "    # au premier passage, on n'a pas de précédente moyenne\n",
    "    if prev_mean is None:\n",
    "        ec = 0.0\n",
    "    else:\n",
    "        ec = abs(current_mean - prev_mean)\n",
    "\n",
    "    ecs.append(ec)\n",
    "\n",
    "    # on met à jour le meilleur écart\n",
    "    if ec > best:\n",
    "        best = ec\n",
    "        best_prop0 = np.linspace(0.5, 1, 100)[i]\n",
    "        best_prop1 = np.linspace(0.5, 1, 100)[i-1]\n",
    "    # on stocke la moyenne courante pour la prochaine itération\n",
    "    prev_mean = current_mean\n",
    "\n",
    "print(f\"Meilleur écart = {best:.4f} obtenu pour prop = {best_prop1:.4f}\")\n",
    "threshold = min(ctr) * best_prop1\n",
    "Y_TARGET =  (df0[\"lastvals\"] < threshold).astype(int).values\n",
    "if Y_TARGET.mean()*100 < 20:\n",
    "    threshold = min(ctr) * best_prop0\n",
    "Y_TARGET =  (df0[\"lastvals\"] < threshold).astype(int).values\n",
    "print(f\"proportion de dropout : {Y_TARGET.mean()}\")\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73a76d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28105268564221186"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cc94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion de dropout : 0.35944299390774587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:57<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_projects                 Modèle  Accuracy  Precision    Recall  \\\n",
      "0            1  Régression Logistique  0.764151   0.717949  0.417910   \n",
      "1            1        Forêt Aléatoire  0.693396   0.520000  0.388060   \n",
      "2            1      Gradient Boosting  0.721698   0.605263  0.343284   \n",
      "3            1           SVM Linéaire  0.726415   0.615385  0.358209   \n",
      "4            2  Régression Logistique  0.745283   0.614035  0.522388   \n",
      "5            2        Forêt Aléatoire  0.646226   0.431034  0.373134   \n",
      "6            2      Gradient Boosting  0.726415   0.588235  0.447761   \n",
      "7            2           SVM Linéaire  0.726415   0.576271  0.507463   \n",
      "8            3  Régression Logistique  0.688679   0.507692  0.492537   \n",
      "9            3        Forêt Aléatoire  0.655660   0.444444  0.358209   \n",
      "10           3      Gradient Boosting  0.698113   0.526316  0.447761   \n",
      "11           3           SVM Linéaire  0.721698   0.568966  0.492537   \n",
      "12           4  Régression Logistique  0.712264   0.544118  0.552239   \n",
      "13           4        Forêt Aléatoire  0.683962   0.500000  0.462687   \n",
      "14           4      Gradient Boosting  0.688679   0.508475  0.447761   \n",
      "15           4           SVM Linéaire  0.707547   0.540984  0.492537   \n",
      "16           5  Régression Logistique  0.712264   0.546875  0.522388   \n",
      "17           5        Forêt Aléatoire  0.716981   0.563636  0.462687   \n",
      "18           5      Gradient Boosting  0.707547   0.540984  0.492537   \n",
      "19           5           SVM Linéaire  0.712264   0.550000  0.492537   \n",
      "20           6  Régression Logistique  0.716981   0.559322  0.492537   \n",
      "21           6        Forêt Aléatoire  0.721698   0.574074  0.462687   \n",
      "22           6      Gradient Boosting  0.707547   0.545455  0.447761   \n",
      "23           6           SVM Linéaire  0.698113   0.523810  0.492537   \n",
      "24           7  Régression Logistique  0.698113   0.523077  0.507463   \n",
      "25           7        Forêt Aléatoire  0.754717   0.653061  0.477612   \n",
      "26           7      Gradient Boosting  0.745283   0.610169  0.537313   \n",
      "27           7           SVM Linéaire  0.688679   0.507937  0.477612   \n",
      "28           8  Régression Logistique  0.891509   0.805556  0.865672   \n",
      "29           8        Forêt Aléatoire  0.971698   0.929577  0.985075   \n",
      "30           8      Gradient Boosting  0.985849   0.957143  1.000000   \n",
      "31           8           SVM Linéaire  0.882075   0.762500  0.910448   \n",
      "\n",
      "    F1 Score   ROC AUC  \n",
      "0   0.528302  0.717447  \n",
      "1   0.444444  0.641071  \n",
      "2   0.438095  0.665003  \n",
      "3   0.452830  0.676737  \n",
      "4   0.564516  0.713021  \n",
      "5   0.400000  0.645033  \n",
      "6   0.508475  0.689552  \n",
      "7   0.539683  0.684097  \n",
      "8   0.500000  0.706845  \n",
      "9   0.396694  0.671230  \n",
      "10  0.483871  0.676222  \n",
      "11  0.528000  0.681472  \n",
      "12  0.548148  0.709933  \n",
      "13  0.480620  0.692434  \n",
      "14  0.476190  0.692486  \n",
      "15  0.515625  0.683788  \n",
      "16  0.534351  0.717241  \n",
      "17  0.508197  0.718785  \n",
      "18  0.515625  0.703603  \n",
      "19  0.519685  0.705198  \n",
      "20  0.523810  0.717756  \n",
      "21  0.512397  0.729542  \n",
      "22  0.491803  0.738343  \n",
      "23  0.507692  0.684508  \n",
      "24  0.515152  0.736902  \n",
      "25  0.551724  0.769738  \n",
      "26  0.571429  0.781678  \n",
      "27  0.492308  0.706639  \n",
      "28  0.834532  0.951518  \n",
      "29  0.956522  0.995677  \n",
      "30  0.978102  0.996397  \n",
      "31  0.829932  0.949151  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import clone\n",
    "\n",
    "def build_X(df_sub: pd.DataFrame, prefixes: list, static_cols: list, n: int) -> np.ndarray:\n",
    "        # on garde student_id + les n premiers items\n",
    "        dyn_cols = [\n",
    "        col for col in df_sub.columns\n",
    "        if any(col.startswith(pref) for pref in prefixes[:n])\n",
    "        ]\n",
    "        keep = [\"email\"] + static_cols + dyn_cols\n",
    "        return df_sub[keep].set_index(\"email\").values\n",
    "\n",
    "def compare_classifiers(X_train, X_test, y_train, y_test, mask_test, models, n, do_plot=False):\n",
    "    \"\"\"\n",
    "    Entraîne plusieurs modèles de classification binaire, calcule leurs métriques de performance\n",
    "    et affiche les courbes ROC.\n",
    "\n",
    "    Args:\n",
    "        X_train (array-like): Caractéristiques d'entraînement\n",
    "        X_test (array-like): Caractéristiques de test\n",
    "        y_train (array-like): Étiquettes d'entraînement\n",
    "        y_test (array-like): Étiquettes de test\n",
    "        models (dict): Dictionnaire {'nom_modèle': instance_modèle}\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Tableau des métriques pour chaque modèle\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for name, mod in (models.items()):\n",
    "        # Entraînement\n",
    "        model = clone(mod)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Prédictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Probabilités ou scores de décision pour ROC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_prob = model.decision_function(X_test)\n",
    "\n",
    "        # puis on ne garde QUE les indices mask_test == True\n",
    "        y_test_r = y_test[mask_test]\n",
    "        y_pred_r = y_pred[mask_test]\n",
    "        y_prob_r = y_prob[mask_test]\n",
    "\n",
    "        # calcul métriques\n",
    "        metrics.append({\n",
    "            \"n_projects\": n,\n",
    "            \"Modèle\":     name,\n",
    "            \"Accuracy\":   accuracy_score(y_test_r, y_pred_r),\n",
    "            \"Precision\":  precision_score(y_test_r, y_pred_r, zero_division=0),\n",
    "            \"Recall\":     recall_score(y_test_r, y_pred_r),\n",
    "            \"F1 Score\":   f1_score(y_test_r, y_pred_r),\n",
    "            \"ROC AUC\":    roc_auc_score(y_test_r, y_prob_r),\n",
    "        })\n",
    "\n",
    "        # Courbe ROC\n",
    "        # fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        # plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "    if do_plot:\n",
    "        # Courbe aléatoire\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Aléatoire')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Courbes ROC des modèles\")\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Tableau des métriques\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    res = []\n",
    "    X_all = df4.copy()\n",
    "    X_all.fillna(0, inplace=True)\n",
    "    mark_cols = [c for c in X_all.columns if c.endswith(\"mark\")][::-1]\n",
    "    prefixes = list(dict.fromkeys(c.rsplit(\"_\",1)[0] for c in mark_cols[::-1]))\n",
    "    static_cols = []\n",
    "    mask_real = (X_all['source'] == 'real').values\n",
    "\n",
    "    df0 = X_all.copy()\n",
    "\n",
    "    def last_marks(row):\n",
    "        vals, cols = [], []\n",
    "        for c in mark_cols:\n",
    "            v = row[c]\n",
    "            if v > 0:\n",
    "                vals.append(v)\n",
    "                cols.append(c)\n",
    "                if len(vals) == 1:\n",
    "                    break\n",
    "        return pd.Series({\"cols\": cols, \"vals\": vals})\n",
    "\n",
    "    tmp = df0.apply(last_marks, axis=1)\n",
    "    df0[[\"last_cols\", \"last_vals\"]] = tmp\n",
    "    df0[\"lastvals\"] = df0[\"last_vals\"].apply(lambda r: r[0])\n",
    "\n",
    "    y = (df0[\"lastvals\"] < threshold).astype(int).values\n",
    "    print(f\"proportion de dropout : {y.mean()}\")\n",
    "    for n in tqdm(range(1, len(prefixes) + 1 )):\n",
    "        X = build_X(X_all,  prefixes, static_cols, n)\n",
    "        # Séparation train/test\n",
    "        X_train, X_test, y_train, y_test, mask_train, mask_test = train_test_split(\n",
    "            X, y, mask_real, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Définition des modèles à comparer\n",
    "        models = {\n",
    "            \"Régression Logistique\": LogisticRegression(max_iter=10000),\n",
    "            \"Forêt Aléatoire\": RandomForestClassifier(n_estimators=1000, random_state=42),\n",
    "            \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "            \"SVM Linéaire\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "        }\n",
    "\n",
    "        # Comparaison\n",
    "        results = compare_classifiers(X_train, X_test, y_train, y_test, mask_test, models, n)\n",
    "        res.append(results)\n",
    "    results_df = pd.concat(res, ignore_index=True)\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afdb243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modèle</th>\n",
       "      <th>mean_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forêt Aléatoire</td>\n",
       "      <td>0.730542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.747642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Régression Logistique</td>\n",
       "      <td>0.741156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM Linéaire</td>\n",
       "      <td>0.732901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Modèle  mean_acc\n",
       "0        Forêt Aléatoire  0.730542\n",
       "1      Gradient Boosting  0.747642\n",
       "2  Régression Logistique  0.741156\n",
       "3           SVM Linéaire  0.732901"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby([ \"Modèle\"]).agg(\n",
    "            mean_acc=(\"Accuracy\", \"mean\"),\n",
    "        ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "038f6bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.243 → 24.9% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7cd7f4acf64b8b9f0a19c071462ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103c86574194854a8e5def7dd81312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765cee31348c4c628f6fc284329be411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.243 → 24.9% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48d655b3228458b96d19c387ea4fc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f81dbba4dda40d4842139fbf3ecb0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a9ac6898c147048a1c25da5c9b7c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.243 → 24.1% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833dceace45248aebae40c4bf99ed6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175cbff62429472b8cee6bd4d674c0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730ddfdbae5a4126a937f65105f7732a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Synthèse conformal prediction ===\n",
      "                                        cov_all_mean  cov_clu_mean  cov_all_w  \\\n",
      "run                              model                                          \n",
      "sans rien                        RF            0.975         0.000      0.974   \n",
      "                                 LR            0.974         0.000      0.972   \n",
      "                                 GB            0.972         0.000      0.970   \n",
      "sans enrichissement              RF            0.975         0.976      0.974   \n",
      "                                 LR            0.974         0.974      0.972   \n",
      "                                 GB            0.972         0.972      0.970   \n",
      "avec enrichissement, technique 2 RF            0.962         0.966      0.955   \n",
      "                                 LR            0.955         0.960      0.957   \n",
      "                                 GB            0.962         0.966      0.957   \n",
      "\n",
      "                                        cov_clu_w  width_all_mean  \\\n",
      "run                              model                              \n",
      "sans rien                        RF         0.000           1.652   \n",
      "                                 LR         0.000           1.698   \n",
      "                                 GB         0.000           1.627   \n",
      "sans enrichissement              RF         0.975           1.652   \n",
      "                                 LR         0.972           1.698   \n",
      "                                 GB         0.971           1.627   \n",
      "avec enrichissement, technique 2 RF         0.960           1.551   \n",
      "                                 LR         0.961           1.671   \n",
      "                                 GB         0.961           1.559   \n",
      "\n",
      "                                        width_clu_mean  width_all_w  \\\n",
      "run                              model                                \n",
      "sans rien                        RF              0.000        1.578   \n",
      "                                 LR              0.000        1.665   \n",
      "                                 GB              0.000        1.561   \n",
      "sans enrichissement              RF              1.659        1.578   \n",
      "                                 LR              1.701        1.665   \n",
      "                                 GB              1.632        1.561   \n",
      "avec enrichissement, technique 2 RF              1.510        1.452   \n",
      "                                 LR              1.625        1.655   \n",
      "                                 GB              1.518        1.478   \n",
      "\n",
      "                                        width_clu_w  \n",
      "run                              model               \n",
      "sans rien                        RF           0.000  \n",
      "                                 LR           0.000  \n",
      "                                 GB           0.000  \n",
      "sans enrichissement              RF           1.586  \n",
      "                                 LR           1.670  \n",
      "                                 GB           1.568  \n",
      "avec enrichissement, technique 2 RF           1.417  \n",
      "                                 LR           1.615  \n",
      "                                 GB           1.436  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tasks = [\n",
    "    (\"sans rien\", df1, {}),\n",
    "    (\"sans enrichissement\", df3, {}),\n",
    "    (\"avec enrichissement, technique 2\", df4, {}),\n",
    "   #  (\"avec enrichissement, technique ACP inverse\", df5, {})\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_cible, tr_clf = run_analysis_w(\n",
    "        df=dfk,\n",
    "        threshold=threshold,\n",
    "        do_plot=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster'] == -1])\n",
    "    # 2) Agrégation conditionnelle\n",
    "    df_cond = aggregate_conformal_metrics(df_detail, dfk)\n",
    "    \n",
    "    # 3) Moyennes pondérées\n",
    "    if wac:\n",
    "        cutoff = 12\n",
    "    elif year == 24:\n",
    "        cutoff = 13\n",
    "    elif year == 23:\n",
    "        cutoff = 1\n",
    "    df_w = compute_weighted_conformal_metrics(df_cond, lmbda=0.9, cutoff=cutoff)\n",
    "    # 4) Collecte des résultats par modèle\n",
    "    for model in df_cond['model'].unique():\n",
    "        summary_records.append({\n",
    "            'run':            name,\n",
    "            'model':          model,\n",
    "            'cov_all_mean':   df_cond.loc[df_cond['model']==model, 'cov_all'].mean(),\n",
    "            'cov_clu_mean':   df_cond.loc[df_cond['model']==model, 'cov_cluster'].mean(),\n",
    "            'cov_all_w':      df_w.loc[model, 'cov_all_w'],\n",
    "            'cov_clu_w':      df_w.loc[model, 'cov_cluster_w'],\n",
    "            'width_all_mean':   df_cond.loc[df_cond['model']==model, 'width_all'].mean(),\n",
    "            'width_clu_mean':   df_cond.loc[df_cond['model']==model, 'width_cluster'].mean(),\n",
    "            'width_all_w':    df_w.loc[model, 'width_all_w'],\n",
    "            'width_clu_w':    df_w.loc[model, 'width_cluster_w'],\n",
    "        })\n",
    "\n",
    "# 5) Construire le DataFrame de synthèse\n",
    "df_summary = pd.DataFrame.from_records(summary_records)\n",
    "df_summary = df_summary.set_index(['run','model'])\n",
    "\n",
    "# 6) Affichage\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "print(\"\\n=== Synthèse conformal prediction ===\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "392dae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla-CP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.972</td>\n",
       "      <td>1.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.972</td>\n",
       "      <td>1.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method model  cluster  mean_coverage  mean_width\n",
       "0  mondrian    GB       -1          0.972       1.627\n",
       "1  mondrian    LR       -1          0.974       1.698\n",
       "2  mondrian    RF       -1          0.975       1.652\n",
       "3   vanilla    GB       -1          0.972       1.627\n",
       "4   vanilla    LR       -1          0.974       1.698\n",
       "5   vanilla    RF       -1          0.975       1.652"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"vanilla-CP\")\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de707c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondrian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.972</td>\n",
       "      <td>1.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method model  cluster  mean_coverage  mean_width\n",
       "0   mondrian    GB       -1          0.974       1.700\n",
       "4   mondrian    LR       -1          0.979       1.746\n",
       "8   mondrian    RF       -1          0.971       1.697\n",
       "12   vanilla    GB       -1          0.972       1.627\n",
       "16   vanilla    LR       -1          0.974       1.698\n",
       "20   vanilla    RF       -1          0.975       1.652"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mondrian\")\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7cd0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondrian + Enrichment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.972</td>\n",
       "      <td>1.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1.551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method model  cluster  mean_coverage  mean_width\n",
       "0   mondrian    GB       -1          0.966       1.609\n",
       "5   mondrian    LR       -1          0.965       1.729\n",
       "10  mondrian    RF       -1          0.972       1.640\n",
       "15   vanilla    GB       -1          0.962       1.559\n",
       "20   vanilla    LR       -1          0.955       1.671\n",
       "25   vanilla    RF       -1          0.962       1.551"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mondrian + Enrichment\")\n",
    "res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5152418",
   "metadata": {},
   "source": [
    "## Enrichment with $G_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "df7613ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import umap\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "dd2501ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wac:\n",
    "    if year == 23:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal_WAC_2023.csv\")\n",
    "    elif year == 24:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal_WAC_2024.csv\")\n",
    "else:\n",
    "    if year == 23:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal2023.csv\")\n",
    "    elif year == 24:\n",
    "        CSV_FILE = Path(\"real_data_fin/merged_horizontal2024.csv\")\n",
    "df_ = pd.read_csv(CSV_FILE)\n",
    "mask = df_.columns.str.startswith(\"B-CPE-210\")\n",
    "df_ = df_.loc[:, ~mask]\n",
    "df = df_.copy()\n",
    "mark_cols = [c for c in df.columns if c.endswith(\"mark\")]\n",
    "df[mark_cols] = df[mark_cols].div(df[mark_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4c4fcf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffixe = cpoolday01 → shape (851, 24)\n",
      "Suffixe = cpoolday02 → shape (851, 22)\n",
      "Suffixe = cpoolday03 → shape (851, 26)\n",
      "Suffixe = cpoolday04 → shape (851, 20)\n",
      "Suffixe = cpoolday05 → shape (851, 24)\n",
      "Suffixe = cpoolday06 → shape (851, 50)\n",
      "Suffixe = cpoolday07 → shape (851, 20)\n",
      "Suffixe = cpoolday08 → shape (851, 18)\n",
      "Suffixe = cpoolday09 → shape (851, 20)\n",
      "Suffixe = cpoolday10 → shape (851, 18)\n",
      "Suffixe = cpoolday11 → shape (851, 30)\n",
      "Suffixe = cpoolday12 → shape (851, 16)\n",
      "Suffixe = cpoolday13 → shape (851, 24)\n",
      "Suffixe = myls → shape (851, 14)\n",
      "Suffixe = settingup → shape (851, 32)\n",
      "Suffixe = mytop → shape (851, 15)\n",
      "Suffixe = organized → shape (851, 13)\n",
      "Suffixe = secured → shape (851, 14)\n",
      "Suffixe = mysudo → shape (851, 11)\n",
      "Suffixe = minishell1 → shape (851, 19)\n",
      "Suffixe = robotfactory → shape (851, 12)\n",
      "Suffixe = minishell2 → shape (851, 25)\n",
      "Suffixe = amazed → shape (851, 14)\n",
      "Suffixe = 42sh → shape (851, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. On récupère la liste des colonnes sans 'email'\n",
    "cols = df.drop(columns=['email']).columns.to_series()\n",
    "\n",
    "# 2. On extrait le \"suffixe\" (ce qui suit le premier '_')\n",
    "suffixes = cols.apply(lambda x: x.split('_')[1])\n",
    "ordered_suffixes = suffixes.unique()\n",
    "# 3. On regroupe les noms de colonnes par suffixe\n",
    "dfs = {}\n",
    "for suffix in ordered_suffixes:\n",
    "    # sélectionne les colonnes dont, une fois splitté, le suffixe correspond\n",
    "    cols_for_suffix = [\n",
    "        col for col in df.columns\n",
    "        if col != 'email' and col.split('_')[1] == suffix\n",
    "    ]\n",
    "    dfs[suffix] = df[cols_for_suffix]\n",
    "\n",
    "\n",
    "# Exemple d’utilisation :\n",
    "for suffix, subdf in dfs.items():\n",
    "    print(f\"Suffixe = {suffix} → shape {subdf.shape}\")\n",
    "    # display(subdf.head())  # si vous êtes en Jupyter / notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "91ece7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "  4%|▍         | 1/24 [00:11<04:23, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday01 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "  8%|▊         | 2/24 [00:23<04:20, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday02 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 12%|█▎        | 3/24 [00:36<04:20, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday03 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 17%|█▋        | 4/24 [00:50<04:17, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday04 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 21%|██        | 5/24 [01:02<03:58, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday05 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 25%|██▌       | 6/24 [01:11<03:27, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday06 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 29%|██▉       | 7/24 [01:24<03:22, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday07 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 33%|███▎      | 8/24 [01:38<03:23, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday08 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 38%|███▊      | 9/24 [01:53<03:19, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday09 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 42%|████▏     | 10/24 [02:09<03:18, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 46%|████▌     | 11/24 [02:23<03:04, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 50%|█████     | 12/24 [02:39<02:56, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 54%|█████▍    | 13/24 [02:55<02:45, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpoolday13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 58%|█████▊    | 14/24 [03:07<02:20, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myls done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 62%|██████▎   | 15/24 [03:16<01:54, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settingup done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 67%|██████▋   | 16/24 [03:25<01:32, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mytop done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 71%|███████   | 17/24 [03:36<01:19, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organized done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 75%|███████▌  | 18/24 [03:46<01:04, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secured done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 79%|███████▉  | 19/24 [03:59<00:57, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysudo done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 83%|████████▎ | 20/24 [04:06<00:40, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minishell1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 88%|████████▊ | 21/24 [04:18<00:31, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robotfactory done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 92%|█████████▏| 22/24 [04:24<00:18,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minishell2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      " 96%|█████████▌| 23/24 [04:34<00:09,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazed done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1887: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n",
      "C:\\Users\\Anton CONRAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "100%|██████████| 24/24 [04:45<00:00, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42sh done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xt = {}\n",
    "for suffix, subdf in tqdm(dfs.items()):\n",
    "    test_cols = [c for c in subdf.columns if c.endswith('passed')]\n",
    "    subdf_tests = subdf[test_cols].fillna(0)\n",
    "    reducer = umap.UMAP(\n",
    "    n_components=3,    \n",
    "    n_neighbors=50,    # nombre de voisins pour la structure locale (par défaut 15)\n",
    "    min_dist=0.1,      # distance minimale entre points dans l'espace réduit\n",
    "    metric='hamming',  # métrique de distance\n",
    "    random_state=42    # pour la reproductibilité\n",
    ")\n",
    "\n",
    "    embedding = reducer.fit_transform(subdf_tests.values)\n",
    "\n",
    "    # 3. Convertir en DataFrame pour plus de commodité\n",
    "    subdf_tests_umap3 = pd.DataFrame(\n",
    "        embedding,\n",
    "        columns=[f\"UMAP_{i+1}_passed\" for i in range(3)],\n",
    "        index=df.index\n",
    "    )\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    array_norm = scaler.fit_transform(subdf_tests_umap3)\n",
    "    subdf_tests_umap3_norm = pd.DataFrame(\n",
    "    array_norm,\n",
    "    index=subdf_tests_umap3.index,\n",
    "    columns=subdf_tests_umap3.columns\n",
    "    )\n",
    "    # print(subdf_tests_umap3.head())\n",
    "    Xt[suffix] =  pd.concat([subdf_tests_umap3_norm, subdf.drop(columns=test_cols)], axis=1, ignore_index=True).fillna(0)\n",
    "    # print(X[suffix].head())\n",
    "    print(suffix, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "29e7729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(Xt.keys())\n",
    "w = 3\n",
    "H = 0\n",
    "X = []  # liste des features (fenêtres concaténées)\n",
    "y = []  # liste des targets\n",
    "\n",
    "for i in range(w, len(keys)):\n",
    "    # clé courante\n",
    "    suffix = keys[i]\n",
    "    # print(suffix)\n",
    "    # 1) Construction de la fenêtre des w précédents\n",
    "    x_i_frames = []\n",
    "    for j in range(1, w+1):\n",
    "        prev_key = keys[i - j]\n",
    "        df_prev = Xt[prev_key]\n",
    "        # supprimer les colonnes 3 et 4\n",
    "        # df_prev = df_prev.drop(df_prev.columns[[3, 4]], axis=1)\n",
    "        x_i_frames.append(df_prev)\n",
    "    \n",
    "    # concatène horizontalement les w DataFrames\n",
    "    X.append(pd.concat(x_i_frames, axis=1))\n",
    "    \n",
    "    # 2) Extraction de la colonne cible (indice 3) du DataFrame courant\n",
    "    if H + i < len(keys):\n",
    "        y_i = Xt[keys[i+H]].iloc[:, 3]\n",
    "    else:\n",
    "        y_i = Xt[keys[-1]].iloc[:, 3]\n",
    "    y.append(y_i)\n",
    "\n",
    "X_array_hori = [df.values for df in X]\n",
    "y_array_hori = [s.values for s in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "aec6db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:   0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:   5%|▌         | 1/19 [00:12<03:48, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  11%|█         | 2/19 [00:42<06:23, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  16%|█▌        | 3/19 [01:24<08:25, 31.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  21%|██        | 4/19 [02:21<10:25, 41.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  26%|██▋       | 5/19 [03:35<12:26, 53.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  32%|███▏      | 6/19 [05:09<14:30, 66.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  37%|███▋      | 7/19 [06:59<16:12, 81.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  42%|████▏     | 8/19 [09:12<17:55, 97.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  47%|████▋     | 9/19 [11:28<18:17, 109.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  53%|█████▎    | 10/19 [14:01<18:28, 123.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  58%|█████▊    | 11/19 [16:51<18:19, 137.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  63%|██████▎   | 12/19 [19:54<17:39, 151.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  68%|██████▊   | 13/19 [23:14<16:35, 165.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  74%|███████▎  | 14/19 [26:53<15:09, 181.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  79%|███████▉  | 15/19 [30:45<13:07, 196.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  84%|████████▍ | 16/19 [34:35<10:21, 207.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  89%|████████▉ | 17/19 [38:42<07:17, 219.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne:  95%|█████████▍| 18/19 [43:02<03:51, 231.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 ok\n",
      "fit 2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fenêtres en ligne: 100%|██████████| 19/19 [47:39<00:00, 150.52s/it]\n"
     ]
    }
   ],
   "source": [
    "X_array = X_array_hori\n",
    "y_array = y_array_hori\n",
    "U_t = []\n",
    "# On parcourt i de 1 à len(X_array)-1 (i=0 n'a pas de passé pour entraîner)\n",
    "for i in tqdm(range(1, len(X_array) - 1), desc=\"Fenêtres en ligne\"):\n",
    "    # --- 1) Construction du train sur les fenêtres passées ---\n",
    "    X_train = np.vstack(X_array[:i])      # fenêtres 0..i-1\n",
    "    y_train = np.concatenate(y_array[:i])\n",
    "    # --- 2) Entraînement d'un nouveau modèle ---\n",
    "    model = OneSidedSPCI_LGBM_Offline(alpha=0.1, w=300, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    X_i, y_i = X_array[i], y_array[i]\n",
    "    # calcul des bornes supérieures U_t pour chaque échantillon de X_i\n",
    "    U = np.array([\n",
    "        model.predict_interval(x.reshape(1, -1))[1]\n",
    "        for x in X_i\n",
    "    ])\n",
    "    U_t.append(U)\n",
    "# print(U_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2e03bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton CONRAD\\AppData\\Local\\Temp\\ipykernel_23148\\1913715893.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  change_points = [i for i in range(len(cols) -1) if suffixes[i]!= suffixes[i+1]][::-1]\n"
     ]
    }
   ],
   "source": [
    "df6 = df1.copy()\n",
    "cols = df6.drop(columns=['email', 'source']).columns.to_series()\n",
    "# 2. On extrait le \"suffixe\" (ce qui suit le premier '_')\n",
    "suffixes = cols.apply(lambda x: x.split('_')[1])\n",
    "ordered_suffixes = suffixes.unique()\n",
    "prefixes = df6.columns.str.split(\"_\").str[0]\n",
    "prefixes2 = df6.drop(columns=['email']).columns.str.split(\"_\").str[:2].str.join(\"_\").unique()[w-1:][::-1]\n",
    "change_points = [i for i in range(len(cols) -1) if suffixes[i]!= suffixes[i+1]][::-1]\n",
    "for i, ut in enumerate(reversed(U_t)):\n",
    "    loc = change_points[i]\n",
    "    col_name = f\"{prefixes2[i]}_next_grade\"\n",
    "    df6.insert(loc - 1, col_name, ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e985d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.242 → 25.0% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e760ddc461c745c08cc0db7a21b666c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c026bff65a6a4796b22831168f3445f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c12b9248bdc476f92d8c5ea29267897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = [\n",
    "    (\"\", df6, {}),\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_cible, tr_clf = run_analysis_w(\n",
    "        df=dfk,\n",
    "        threshold=threshold,\n",
    "        do_plot=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster'] == -1])\n",
    "    # 2) Agrégation conditionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ef18cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla + SPCI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method model  cluster  mean_coverage  mean_width\n",
       "0  mondrian    GB       -1          0.949       1.549\n",
       "1  mondrian    LR       -1          0.968       1.695\n",
       "2  mondrian    RF       -1          0.947       1.534\n",
       "3   vanilla    GB       -1          0.949       1.549\n",
       "4   vanilla    LR       -1          0.968       1.695\n",
       "5   vanilla    RF       -1          0.947       1.534"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vanilla + SPCI\")\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6fa7155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    257\n",
      "1    466\n",
      "2     13\n",
      "3    115\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    # CPE 2024\n",
    "    dfcpool = df1[[c for c in df1.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday\\d+_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    dfcpool_mark = dfcpool[cols_keep]\n",
    "    X = dfcpool_mark.fillna(0)\n",
    "elif year == 23:\n",
    "    # CPE 2023\n",
    "    X = df1[[c for c in df1.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df7 = df6.copy()\n",
    "df7['cluster'] = clusters\n",
    "print(pd.Series(clusters).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7cd42cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters trop petits à réaffecter : [2]\n",
      "Nouvelles tailles de clusters :\n",
      " 0    477\n",
      "1    130\n",
      "3    244\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "min_cluster_size = 50\n",
    "n_clusters       = 4\n",
    "\n",
    "if wac:\n",
    "    # Données WAC (WEB)\n",
    "    X = df2[[c for c in df2.columns if c.startswith(\"W-WEB-024\")]].fillna(0)\n",
    "elif year == 24:\n",
    "    dfcpool = df2[[c for c in df2.columns if c.startswith(\"B-CPE-100\")]]\n",
    "    pat = re.compile(r\"B-CPE-100_cpoolday01_\\d{2} - task\\d+_passed\")\n",
    "    cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "    X = dfcpool[cols_keep].fillna(0)\n",
    "elif year == 23:\n",
    "    X = df2[[c for c in df2.columns if c.startswith(\"B-CPE-110_settingup\")]].fillna(0)\n",
    "else:\n",
    "    raise ValueError(f\"Année inattendue : {year}\")\n",
    "\n",
    "# 2) Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3) Premier KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# 4) Mesure des tailles\n",
    "sizes = pd.Series(labels).value_counts().sort_index()\n",
    "\n",
    "# 5) Réaffectation des points de petits clusters\n",
    "small_clusters = sizes[sizes < min_cluster_size].index.tolist()\n",
    "if small_clusters:\n",
    "    print(f\"Clusters trop petits à réaffecter : {small_clusters}\")\n",
    "    # Pour chaque point des clusters trop petits, on calcule la distance à tous les centroides\n",
    "    for sc in small_clusters:\n",
    "        mask_sc = labels == sc\n",
    "        idxs   = np.where(mask_sc)[0]\n",
    "        for i in idxs:\n",
    "            # distances à TOUTES les centroïdes\n",
    "            dists = np.linalg.norm(X_scaled[i] - centroids, axis=1)\n",
    "            # on interdit de rester dans le même (petit) cluster\n",
    "            dists[sc] = np.inf\n",
    "            # réaffectation vers le plus proche cluster valide\n",
    "            labels[i] = int(np.argmin(dists))\n",
    "    # Optionnel : recompute sizes\n",
    "    sizes = pd.Series(labels).value_counts().sort_index()\n",
    "    print(\"Nouvelles tailles de clusters :\\n\", sizes)\n",
    "\n",
    "df8 = df7.copy()\n",
    "df8[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f3b31ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0    257\n",
      "1    466\n",
      "2    104\n",
      "3    115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- 0) Préparation de X et des colonnes ---\n",
    "# X contient déjà 'cluster' et toutes les features, y compris les binaires\n",
    "X = df7.copy()\n",
    "X = X.drop(columns=['email', 'source']).fillna(0).reset_index(drop=True)\n",
    "clusters    = X['cluster'].values\n",
    "feature_cols = X.columns.drop('cluster')\n",
    "\n",
    "# distinguez numériques / cat\n",
    "style_cols = [c for c in feature_cols if \"_style\" in c]\n",
    "bin_cols = [c for c in feature_cols if c.endswith('passed')] + style_cols\n",
    "num_cols = [c for c in feature_cols if c not in bin_cols]\n",
    "\n",
    "# --- 1) Calcul des probabilités par cluster pour chaque binaire ---\n",
    "cluster_probs = (\n",
    "    X\n",
    "    .groupby('cluster')[bin_cols]\n",
    "    .mean()  # pour chaque cluster, freq de 1 dans chaque colonne binaire\n",
    ")\n",
    "dfy = X.copy()\n",
    "# --- 2) Paramètres d’augmentation ---\n",
    "minor_counts = X['cluster'].value_counts()\n",
    "th    = minor_counts.quantile(0.25)  # on cible les 25% les moins gros\n",
    "minor_cs     = minor_counts[minor_counts < th].index.tolist()\n",
    "\n",
    "# --- 3) Génération des nouveaux points ---\n",
    "rows_aug = []\n",
    "\n",
    "for cl in minor_cs:\n",
    "    idx = np.where(clusters == cl)[0]\n",
    "    Xk  = X.loc[idx, feature_cols].values\n",
    "    n_new_per    = 100 // len(idx)\n",
    "    k_neighbors  = min(20, len(idx)-2)\n",
    "    # si trop peu de points, on duplique\n",
    "    if len(Xk) < 2:\n",
    "        for i in idx:\n",
    "            for _ in range(n_new_per):\n",
    "                rows_aug.append(X.loc[i, feature_cols].to_dict())\n",
    "        continue\n",
    "\n",
    "    # on calcule les voisins sur la partie numérique\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=min(k_neighbors+1, len(Xk)),\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    nbrs.fit(Xk[:, [X.columns.get_loc(c) for c in num_cols]])\n",
    "    neigh_idxs = nbrs.kneighbors(return_distance=False)\n",
    "\n",
    "    for i, xi in enumerate(Xk):\n",
    "        for _ in range(n_new_per):\n",
    "            # choix d'un voisin différent\n",
    "            nbr_list = [j for j in neigh_idxs[i] if j != i]\n",
    "            j = np.random.choice(nbr_list)\n",
    "            xj = Xk[j]\n",
    "\n",
    "            # interpolation SMOTE-like pour num_cols\n",
    "            lam = np.random.rand()\n",
    "            num_new = xi[[X.columns.get_loc(c) for c in num_cols]] + \\\n",
    "                      lam * (xj[[X.columns.get_loc(c) for c in num_cols]] - \n",
    "                             xi[[X.columns.get_loc(c) for c in num_cols]])\n",
    "\n",
    "            # génération des bin_cols selon cluster_probs\n",
    "            bin_new = {}\n",
    "            for c in bin_cols:\n",
    "                p = cluster_probs.loc[cl, c]\n",
    "                # on tire un 1 avec prob = p, sinon 0\n",
    "                bin_new[c] = int(np.random.rand() < p)\n",
    "\n",
    "            # assemblage de la nouvelle ligne\n",
    "            new_row = {c: num_new[k] for k, c in enumerate(num_cols)}\n",
    "            new_row.update(bin_new)\n",
    "            new_row['cluster'] = cl\n",
    "            rows_aug.append(new_row)\n",
    "\n",
    "# --- 4) Création du DataFrame synthétique et concaténation ---\n",
    "df_synth = pd.DataFrame(rows_aug)\n",
    "# y_synth = y_synth = (np.random.rand(len(df_synth))  < df_synth['cluster'].map(cluster_y_rate)).astype(int)\n",
    "\n",
    "df_synth['source'] = \"synth\"\n",
    "df_synth['email'] = np.nan\n",
    "df9 = pd.concat([df7, df_synth], ignore_index=True)\n",
    "# y_final = np.concatenate([y_cible, y_synth])\n",
    "print(pd.Series(df9['cluster']).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "acfc3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.242 → 25.0% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544107d792a440ef8150d27b74870940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18755d244f06428da1dec8a26c1dff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f7d305b8e1414ba07e3fd573193990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold  = 0.242 → 23.2% positives\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdbe3ab49d24ffa8defed1ba214e80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RF:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b1020ba394582aa33d57ad233c5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LR:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087345bbfb6f4b349d4da559ec748fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GB:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Synthèse conformal prediction ===\n",
      "                                         cov_all_mean  cov_clu_mean  \\\n",
      "run                               model                               \n",
      "avec clustering                   RF            0.947         0.948   \n",
      "                                  LR            0.968         0.969   \n",
      "                                  GB            0.949         0.950   \n",
      "avec enrichissement et clustering RF            0.959         0.962   \n",
      "                                  LR            0.960         0.964   \n",
      "                                  GB            0.957         0.961   \n",
      "\n",
      "                                         cov_all_w  cov_clu_w  width_all_mean  \\\n",
      "run                               model                                         \n",
      "avec clustering                   RF         0.950      0.949           1.534   \n",
      "                                  LR         0.971      0.972           1.695   \n",
      "                                  GB         0.955      0.955           1.549   \n",
      "avec enrichissement et clustering RF         0.952      0.956           1.550   \n",
      "                                  LR         0.950      0.955           1.639   \n",
      "                                  GB         0.947      0.951           1.536   \n",
      "\n",
      "                                         width_clu_mean  width_all_w  \\\n",
      "run                               model                                \n",
      "avec clustering                   RF              1.539        1.456   \n",
      "                                  LR              1.702        1.677   \n",
      "                                  GB              1.556        1.485   \n",
      "avec enrichissement et clustering RF              1.509        1.463   \n",
      "                                  LR              1.595        1.555   \n",
      "                                  GB              1.495        1.430   \n",
      "\n",
      "                                         width_clu_w  \n",
      "run                               model               \n",
      "avec clustering                   RF           1.460  \n",
      "                                  LR           1.685  \n",
      "                                  GB           1.492  \n",
      "avec enrichissement et clustering RF           1.424  \n",
      "                                  LR           1.513  \n",
      "                                  GB           1.392  \n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    (\"avec clustering\", df8, {}),\n",
    "     (\"avec enrichissement et clustering\", df9, {})\n",
    "]\n",
    "\n",
    "summary_records = []\n",
    "res = []\n",
    "for name, dfk, kwargs in tasks:\n",
    "    # 1) Lancement de la fonction\n",
    "    df_detail, df_agg, y_cible, tr_clf = run_analysis_w(\n",
    "        df=dfk,\n",
    "        threshold=threshold,\n",
    "        do_plot=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    r =  df_detail.groupby([\"method\", \"model\", \"cluster\"]).agg(\n",
    "            mean_coverage=(\"coverage\", \"mean\"),\n",
    "            mean_width=(\"width\", \"mean\")\n",
    "        ).reset_index()\n",
    "    res.append(r[r['cluster'] == -1])\n",
    "    # 2) Agrégation conditionnelle\n",
    "    df_cond = aggregate_conformal_metrics(df_detail, dfk)\n",
    "    \n",
    "    # 3) Moyennes pondérées\n",
    "    if wac:\n",
    "        cutoff = 12\n",
    "    elif year == 24:\n",
    "        cutoff = 13\n",
    "    elif year == 23:\n",
    "        cutoff = 1\n",
    "    df_w = compute_weighted_conformal_metrics(df_cond, lmbda=0.9, cutoff=cutoff)\n",
    "    # 4) Collecte des résultats par modèle\n",
    "    for model in df_cond['model'].unique():\n",
    "        summary_records.append({\n",
    "            'run':            name,\n",
    "            'model':          model,\n",
    "            'cov_all_mean':   df_cond.loc[df_cond['model']==model, 'cov_all'].mean(),\n",
    "            'cov_clu_mean':   df_cond.loc[df_cond['model']==model, 'cov_cluster'].mean(),\n",
    "            'cov_all_w':      df_w.loc[model, 'cov_all_w'],\n",
    "            'cov_clu_w':      df_w.loc[model, 'cov_cluster_w'],\n",
    "            'width_all_mean':   df_cond.loc[df_cond['model']==model, 'width_all'].mean(),\n",
    "            'width_clu_mean':   df_cond.loc[df_cond['model']==model, 'width_cluster'].mean(),\n",
    "            'width_all_w':    df_w.loc[model, 'width_all_w'],\n",
    "            'width_clu_w':    df_w.loc[model, 'width_cluster_w'],\n",
    "        })\n",
    "\n",
    "# 5) Construire le DataFrame de synthèse\n",
    "df_summary = pd.DataFrame.from_records(summary_records)\n",
    "df_summary = df_summary.set_index(['run','model'])\n",
    "\n",
    "# 6) Affichage\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "print(\"\\n=== Synthèse conformal prediction ===\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8dd97e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondrian + SPCI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method model  cluster  mean_coverage  mean_width\n",
       "0   mondrian    GB       -1          0.963       1.636\n",
       "4   mondrian    LR       -1          0.982       1.789\n",
       "8   mondrian    RF       -1          0.950       1.605\n",
       "12   vanilla    GB       -1          0.949       1.549\n",
       "16   vanilla    LR       -1          0.968       1.695\n",
       "20   vanilla    RF       -1          0.947       1.534"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mondrian + SPCI\")\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eb2a423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mondrian + Enrichment + SPCI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_coverage</th>\n",
       "      <th>mean_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.970</td>\n",
       "      <td>1.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mondrian</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>GB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>LR</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>RF</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1.550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method model  cluster  mean_coverage  mean_width\n",
       "0   mondrian    GB       -1          0.969       1.643\n",
       "5   mondrian    LR       -1          0.970       1.739\n",
       "10  mondrian    RF       -1          0.969       1.659\n",
       "15   vanilla    GB       -1          0.957       1.536\n",
       "20   vanilla    LR       -1          0.960       1.639\n",
       "25   vanilla    RF       -1          0.959       1.550"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mondrian + Enrichment + SPCI\")\n",
    "res[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
