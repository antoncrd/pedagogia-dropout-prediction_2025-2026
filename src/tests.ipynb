{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from mapie.metrics import (\n",
    "    classification_coverage_score,\n",
    "    classification_mean_width_score\n",
    ")\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.model_production_data_processing_utils import cluster_with_min_size\n",
    "\n",
    "root = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f047847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_production_main import load_and_preprocess_data, prepare_features\n",
    "from utils.model_production_data_processing_utils import compute_threshold_kmeans, build_X_s, build_umap_windows_by_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPCI_lg_to_set(G, threshold):\n",
    "    L, U = G\n",
    "    if U < threshold:\n",
    "        return [1]\n",
    "    elif L > threshold:\n",
    "        return [0]\n",
    "    else:\n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7612e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = joblib.load(root / \"models\" / \"models_SPCI_lg_24.joblib\")\n",
    "mod.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b852038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb704629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labelsets(pset_matrix: np.ndarray):\n",
    "    \"\"\"\n",
    "    Mappe chaque ligne (inclusions par classe) vers l'un de {[0], [1], [0,1]}.\n",
    "    On considère toute valeur non nulle comme 'inclus'.\n",
    "    Fallback conservateur -> [0,1] si ensemble vide.\n",
    "    \"\"\"\n",
    "    ps = (pset_matrix != 0)  # booléen\n",
    "    # sécurités si la seconde dimension n'est pas exactement 2\n",
    "    if ps.ndim != 2 or ps.shape[1] < 2:\n",
    "        raise ValueError(f\"pset_matrix doit être de forme (n_samples, 2), reçu {ps.shape}\")\n",
    "\n",
    "    out = []\n",
    "    for inc0, inc1 in ps[:, 0], ps[:, 1]:\n",
    "        if inc0 and inc1:\n",
    "            out.append([0, 1])\n",
    "        elif inc0:\n",
    "            out.append([0])\n",
    "        elif inc1:\n",
    "            out.append([1])\n",
    "        else:\n",
    "            # très rare (ensemble vide) : fallback conservateur\n",
    "            out.append([0, 1])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_and_preprocess_data(root / \"data/DATA.csv\", 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.read_csv(root / \"data/y_true_24\")\n",
    "mark_cols = [c for c in df1.columns if c.endswith(\"mark\")]\n",
    "prefixes = list(dict.fromkeys(c.rsplit(\"_\",1)[0] for c in mark_cols))\n",
    "static_cols = []\n",
    "\n",
    "threshold = compute_threshold_kmeans(df1)\n",
    "# Prepare features\n",
    "X = prepare_features(df1, 24)\n",
    "\n",
    "# Perform clustering\n",
    "df2, info = cluster_with_min_size(\n",
    "    df1, X, n_clusters=4, min_cluster_size=50, random_state=42\n",
    ")\n",
    "\n",
    "mod1_obj = joblib.load(root / \"models\" / \"models_clustering_24.joblib\")\n",
    "mod2_obj = joblib.load(root / \"models\" / \"models_clustering_24.joblib\")\n",
    "alpha = 0.1\n",
    "w1 = 3\n",
    "\n",
    "covs = {}\n",
    "wids = {}\n",
    "Xt, keys, X_arr, y_arr = build_umap_windows_by_suffix(\n",
    "        df1, w=w1, H=0, target_col_idx=3, verbose=True\n",
    "    )\n",
    "\n",
    "for base_model in ['RF', 'GB']:\n",
    "    for n  in range(1, 17):\n",
    "        key = (base_model, n, \"vanilla\")\n",
    "        x = build_X_s(df2.fillna(0), prefixes, static_cols, n)\n",
    "        model = mod1_obj[key]\n",
    "        yp_van, yps_van = model.predict(x, alpha=alpha) # partition=df2['clusters'])\n",
    "        pset_van = yps_van[:, :, 0]\n",
    "        cov = classification_coverage_score(y_true, pset_van)\n",
    "        wid = classification_mean_width_score(pset_van)\n",
    "        covs[(base_model, n)] = cov\n",
    "        wids[(base_model, n)] = wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0645a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emails = df2['email'].astype(str).values\n",
    "\n",
    "pred_dfs = {}  # contiendra un DF par base_model (RF, GB)\n",
    "\n",
    "for base_model in ['RF', 'GB']:\n",
    "    cols = {}\n",
    "    for n in range(1, 17):\n",
    "        key = (base_model, n, \"vanilla\")\n",
    "        x_n = build_X_s(df2.fillna(0), prefixes, static_cols, n)\n",
    "\n",
    "        model = mod1_obj[key]\n",
    "        _yp_van, yps_van = model.predict(x_n, alpha=alpha)\n",
    "\n",
    "        # pset_van = ensemble(s) prédictif(s) pour chaque étudiant\n",
    "        # On le stocke tel quel ; .tolist() garantit un objet sérialisable (scalaires ou listes)\n",
    "        pset_van = yps_van[:, :, 0]\n",
    "        cols[n] = [[0,1] if (a and b) else [0] if a else [1] if b else [0,1]\n",
    "           for a, b in pset_van]\n",
    "\n",
    "    df_pred = pd.DataFrame(cols, index=emails)\n",
    "    df_pred.index.name = 'email'\n",
    "    pred_dfs[base_model] = df_pred\n",
    "\n",
    "# → Deux dataframes séparés (colonnes = n)\n",
    "preds_RF = pred_dfs['RF']\n",
    "preds_GB = pred_dfs['GB']\n",
    "\n",
    "# (Optionnel) Tout regrouper en colonnes MultiIndex (niveau 0 = base_model, niveau 1 = n)\n",
    "preds_all = pd.concat(pred_dfs, axis=1)  # colonnes comme ('RF', 1), ('RF', 2), ..., ('GB', 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds_RF.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_GB.head(20)\n",
    "# preds_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219369ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(root / \"data/DATA_SPCI_ng_24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28828e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.models_production_utils import build_X_s\n",
    "\n",
    "def gate_predict_minimal(\n",
    "    dataframe, X_arr, n, base_model,\n",
    "    models_c_ng, models_lg, models_comb,\n",
    "    threshold, w2, prefixes, static_cols,\n",
    "    alpha=0.05, partition=None  # partition=df['clusters'] si Mondrian\n",
    "):\n",
    "    \"\"\"\n",
    "    Renvoie:\n",
    "      - p_final: bool array (n_samples, 2)  -> p-set final (après gate)\n",
    "      - choice: int array (0=MCP, 1=SPCI, 2=union)\n",
    "      - y_hat: int array -> 0/1 si singleton, -1 si ambigu (union des deux)\n",
    "    \"\"\"\n",
    "    # 1) Features pour chaque “branche”\n",
    "    X_CP = build_X_s(dataframe, prefixes, static_cols, n)      # mêmes colonnes/ordre qu’à l’entraînement\n",
    "    idx_spci = n - w2\n",
    "    X_SPCI = X_arr[idx_spci]\n",
    "\n",
    "    # 2) Récupérer les modèles\n",
    "    key_mcp = (base_model, n, \"vanilla\")  # ou \"mondrian\" si vous avez entraîné comme tel\n",
    "    model_mcp = models_c_ng[key_mcp]\n",
    "    model_spc = models_lg[n]\n",
    "    gate = models_comb[(base_model, n)]\n",
    "\n",
    "    # 3) p-sets MCP via MAPIE\n",
    "    if partition is None:\n",
    "        y_pred_mcp_gate, yps_mcp_gate = model_mcp.predict(X_CP, alpha=alpha)\n",
    "    else:\n",
    "        y_pred_mcp_gate, yps_mcp_gate = model_mcp.predict(X_CP, alpha=alpha, partition=partition)\n",
    "    p_mcp = yps_mcp_gate[:, :, 0].astype(bool)  # (n_samples, 2)\n",
    "\n",
    "    # 4) p-sets SPCI à partir des intervalles [L,U] et du threshold\n",
    "    intervals = np.array([model_spc.predict_interval(x) for x in X_SPCI], dtype=float)\n",
    "    L_cal = intervals[:, 0]\n",
    "    U_cal = intervals[:, 1]\n",
    "    p_spc = np.zeros_like(p_mcp, dtype=bool)\n",
    "    p_spc[threshold < L_cal, 0] = True\n",
    "    p_spc[threshold > U_cal, 1] = True\n",
    "    amb = ~( (threshold < L_cal) | (threshold > U_cal) )\n",
    "    p_spc[amb, :] = True  # ambigu → {0,1}\n",
    "\n",
    "    # 5) Features pour la gate (mêmes que training): X_CP + [w_cls, w_spc, diff]\n",
    "    w_cls = p_mcp.sum(axis=1)\n",
    "    w_spc = p_spc.sum(axis=1)\n",
    "    diff = w_cls - w_spc\n",
    "    X_gate = np.hstack([X_CP, w_cls.reshape(-1,1), w_spc.reshape(-1,1), diff.reshape(-1,1)])\n",
    "\n",
    "    # 6) Décision de la gate: 0=MCP, 1=SPCI, 2=union\n",
    "    choice = gate.predict(X_gate)\n",
    "\n",
    "    # 7) Composer le p-set final selon la gate\n",
    "    p_final = np.empty_like(p_mcp, dtype=bool)\n",
    "    use_mcp = (choice == 0)\n",
    "    use_spc = (choice == 1)\n",
    "    use_uni = (choice == 2)\n",
    "    p_final[use_mcp] = p_mcp[use_mcp]\n",
    "    p_final[use_spc] = p_spc[use_spc]\n",
    "    p_final[use_uni] = (p_mcp[use_uni] | p_spc[use_uni])\n",
    "\n",
    "    # 8) Étiquette ponctuelle minimale: 0/1 si singleton, sinon -1 (ambigu)\n",
    "    singletons = (p_final.sum(axis=1) == 1)\n",
    "    y_hat = np.where(singletons, p_final.argmax(axis=1), -1)\n",
    "\n",
    "    return {\n",
    "        \"p_final\": p_final,   # bools shape (n_samples, 2)\n",
    "        \"choice\": choice,     # 0/1/2\n",
    "        \"y_hat\": y_hat        # 0/1 ou -1 si ambigu\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = joblib.load(root / \"models\" / \"models_clustering_24.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = joblib.load(root / \"models\" / \"models_clustering_SPCI_ng_24.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(root / \"data/DATA_SPCI_ng_24.csv\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(root / \"data/DATA.csv\")\n",
    "nb_nan_par_ligne = df.isna().sum(axis=1)\n",
    "\n",
    "df = df[nb_nan_par_ligne < 495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d47d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_s(df_sub: pd.DataFrame, prefixes: list, static_cols: list, n: int) -> np.ndarray:\n",
    "    # on garde student_id + les n premiers items\n",
    "    dyn_cols = [\n",
    "    col for col in df_sub.columns\n",
    "    if any(col.startswith(pref) for pref in prefixes[:n])\n",
    "    ]\n",
    "    keep = [\"email\"] + static_cols + dyn_cols\n",
    "    return df_sub[keep].set_index(\"email\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79746372",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_cols = [c for c in df.columns if c.endswith(\"mark\")]\n",
    "prefixes = list(dict.fromkeys(c.rsplit(\"_\",1)[0] for c in mark_cols))\n",
    "static_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_X_s(df.fillna(0), prefixes, static_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpool = df[[c for c in df.columns if c.startswith(\"B-CPE-100\")]]\n",
    "pat = re.compile(r\"B-CPE-100_cpoolday\\d+_\\d{2} - task\\d+_passed\")\n",
    "cols_keep = [c for c in dfcpool.columns if not pat.match(c)]\n",
    "dfcpool_mark = dfcpool[cols_keep]\n",
    "X_pool = dfcpool_mark.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, info = cluster_with_min_size(\n",
    "    df, X_pool, n_clusters=4, min_cluster_size=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for n in range(1, 16):\n",
    "    mod = obj2[('GB', n, 'vanilla')]\n",
    "    X = build_X_s(df2.fillna(0), prefixes, static_cols, n)\n",
    "    yp_van, yps_van = mod.predict(X, alpha=0.1) # partition=df2['clusters'])\n",
    "    pset_van = yps_van[:, :, 0]\n",
    "    print(classification_mean_width_score(pset_van))\n",
    "    res.append(classification_mean_width_score(pset_van))\n",
    "print(\"moy\", np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_series = df.drop(columns=['email']).columns.to_series()\n",
    "suffixes = col_series.apply(lambda x: x.split(\"_\")[1])\n",
    "ordered_suffixes = suffixes.unique()\n",
    "# 2) Groupement des colonnes par suffixe\n",
    "dfs = {}\n",
    "for suffix in ordered_suffixes:\n",
    "    cols_for_suffix = [c for c in col_series if c.split(\"_\")[1] == suffix]\n",
    "    subdf = df[cols_for_suffix].copy()\n",
    "    dfs[suffix] = subdf\n",
    "    if True:\n",
    "        print(f\"Suffixe = {suffix} → shape {subdf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0122c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20  # nombre de colonnes à afficher\n",
    "print(df.isna().sum().sort_values(ascending=False).head(n))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
